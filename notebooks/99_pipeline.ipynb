{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a587774",
   "metadata": {},
   "source": [
    "# Pipeline Runner\n",
    "\n",
    "Run utilities and pipeline steps for selected projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738d590",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d3ae1",
   "metadata": {},
   "source": [
    "### 0.01 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "542848c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import our config\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.config import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4bc20",
   "metadata": {},
   "source": [
    "### 0.02 Load Configuration and Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04bcba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment variables loaded\n",
      "  API Key: sk-proj-cj...__0A\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "project_root = Path.cwd().parent\n",
    "env_path = project_root / \".env\"\n",
    "\n",
    "if not env_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"'.env' file not found at {env_path}\\n\"\n",
    "        \"Please copy .env.example to .env and add your OpenAI API key.\"\n",
    "    )\n",
    "\n",
    "# Load from specific path\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "# Load project config\n",
    "config = load_config()\n",
    "\n",
    "# Get OpenAI API key from environment\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Verify API key is set\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Missing required environment variable: OPENAI_API_KEY\")\n",
    "\n",
    "print(\"✓ Environment variables loaded\")\n",
    "print(f\"  API Key: {OPENAI_API_KEY[:10]}...{OPENAI_API_KEY[-4:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f72ffc",
   "metadata": {},
   "source": [
    "### 0.03 Set Up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c038ef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project data directory: /Users/lauren/repos/PAD2Skills/data/bronze/project_data\n",
      "All project details path: /Users/lauren/repos/PAD2Skills/data/bronze/project_data/all_project_details.csv\n",
      "Project summary path: /Users/lauren/repos/PAD2Skills/data/bronze/project_data/project_summary.csv\n",
      "Files exist: True / True\n"
     ]
    }
   ],
   "source": [
    "# Get paths\n",
    "project_data_dir = project_root / \"data\" / \"bronze\" / \"project_data\"\n",
    "all_project_details_path = project_data_dir / \"all_project_details.csv\"\n",
    "project_summary_path = project_data_dir / \"project_summary.csv\"\n",
    "\n",
    "print(f\"Project data directory: {project_data_dir}\")\n",
    "print(f\"All project details path: {all_project_details_path}\")\n",
    "print(f\"Project summary path: {project_summary_path}\")\n",
    "print(f\"Files exist: {all_project_details_path.exists()} / {project_summary_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1428bc",
   "metadata": {},
   "source": [
    "## 1. Get Project Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2d9d6",
   "metadata": {},
   "source": [
    "### 1.01 Load Project Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84307135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All project details: 123 rows, 20 columns\n",
      "Project summary: 123 rows, 4 columns\n"
     ]
    }
   ],
   "source": [
    "# Read project details and summary\n",
    "all_project_details = pd.read_csv(all_project_details_path)\n",
    "project_summary = pd.read_csv(project_summary_path)\n",
    "\n",
    "print(f\"All project details: {all_project_details.shape[0]} rows, {all_project_details.shape[1]} columns\")\n",
    "print(f\"Project summary: {project_summary.shape[0]} rows, {project_summary.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b97e31",
   "metadata": {},
   "source": [
    "### 1.02 Convert column names to snake_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a0bbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names after standardization:\n",
      "['project_id', 'status', 'team_leader', 'borrower_2', 'country', 'disclosure_date', 'approval_date', 'effective_date', 'total_project_cost_1', 'implementing_agency', 'region', 'fiscal_year_3', 'commitment_amount', 'environmental_category', 'environmental_and_social_risk', 'closing_date', 'last_stage_reached', 'last_update_date', 'consultant_services_required', 'associated_projects']\n"
     ]
    }
   ],
   "source": [
    "# Convert column names to snake_case\n",
    "all_project_details.columns = (\n",
    "    all_project_details.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(' ', '_')\n",
    "    .str.replace(r'[^\\w]', '_', regex=True)\n",
    "    .str.replace(r'_+', '_', regex=True)\n",
    "    .str.strip('_')\n",
    ")\n",
    "\n",
    "print(\"Column names after standardization:\")\n",
    "print(list(all_project_details.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b070f",
   "metadata": {},
   "source": [
    "### 1.03 Merge and Filter Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ebd1d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data: 123 rows, 23 columns\n",
      "Projects with downloaded PADs: 98 rows\n",
      "\n",
      "First few projects:\n",
      "  project_id  status                                        team_leader  \\\n",
      "1    P119893  Closed                    Abdulhakim Mohammed Abdisubhan    \n",
      "3    P173506  Active  Didier Makoso Tsasa , Fabrice Karl Bertholet, ...   \n",
      "4    P176731  Active  Janina Franco , Abdulhakim Mohammed Abdisubhan...   \n",
      "5    P507759  Active                     Jenny Jing Chao , Maria Arango   \n",
      "6    P180547  Active   Monali Ranade , Dana Rysankova, Alona Kazantseva   \n",
      "\n",
      "                                          borrower_2  \\\n",
      "1            Federal Democratic Republic of Ethiopia   \n",
      "3                       DEMOCRATIC REPUBLIC OF CONGO   \n",
      "4            Federal Democratic Republic of Ethiopia   \n",
      "5                             Republic of Mozambique   \n",
      "6  Common Market for Eastern and Southern Africa ...   \n",
      "\n",
      "                         country    disclosure_date  \\\n",
      "1                       Ethiopia  December 22, 2011   \n",
      "3  Congo, Democratic Republic of    January 6, 2021   \n",
      "4                       Ethiopia    August 15, 2023   \n",
      "5                     Mozambique  November 29, 2024   \n",
      "6    Eastern and Southern Africa       July 7, 2023   \n",
      "\n",
      "                approval_date   effective_date  total_project_cost_1  \\\n",
      "1  (as of board presentation)  January 4, 2013    US$ 275.00 million   \n",
      "3  (as of board presentation)     May 18, 2023    US$ 939.00 million   \n",
      "4  (as of board presentation)    June 19, 2024    US$ 537.00 million   \n",
      "5  (as of board presentation)  August 21, 2025      US$ 0.00 million   \n",
      "6  (as of board presentation)    April 5, 2024  US$ 10000.00 million   \n",
      "\n",
      "                                 implementing_agency  ...  \\\n",
      "1  Development Bank of Ethiopia,Ethiopia Electric...  ...   \n",
      "3  Ministère des Ressources Hydrauliques et de l'...  ...   \n",
      "4  Ethiopia Electric Power,Ethiopia Electric Utility  ...   \n",
      "5  Electricidade de Moçambique (EDM),Fundo de Ene...  ...   \n",
      "6  Common Market for Eastern and Southern Africa ...  ...   \n",
      "\n",
      "  environmental_category  environmental_and_social_risk        closing_date  \\\n",
      "1                      B                 Not Applicable      March 31, 2025   \n",
      "3                    NaN                           High  September 30, 2029   \n",
      "4                    NaN                           High     August 30, 2030   \n",
      "5                    NaN                    Substantial   December 31, 2030   \n",
      "6                    NaN                    Substantial   December 31, 2030   \n",
      "\n",
      "  last_stage_reached last_update_date consultant_services_required  \\\n",
      "1      Bank Approved    June 20, 2024                           No   \n",
      "3      Bank Approved    June 17, 2024                          Yes   \n",
      "4      Bank Approved    June 16, 2023                           No   \n",
      "5      Bank Approved   March 22, 2025                          TBD   \n",
      "6      Bank Approved    July 16, 2023                          Yes   \n",
      "\n",
      "  associated_projects details_found pads_found pads_downloaded  \n",
      "1             P155563             1          1               1  \n",
      "3                 NaN             1          1               1  \n",
      "4                 NaN             1          1               1  \n",
      "5             P512418             1          1               1  \n",
      "6                 NaN             1          1               1  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge project_summary to all_project_details\n",
    "all_projects = all_project_details.merge(\n",
    "    project_summary,\n",
    "    on=\"project_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Merged data: {all_projects.shape[0]} rows, {all_projects.shape[1]} columns\")\n",
    "\n",
    "# Keep only projects with downloaded PADs\n",
    "all_projects = all_projects[all_projects[\"pads_downloaded\"] >= 1]\n",
    "\n",
    "print(f\"Projects with downloaded PADs: {all_projects.shape[0]} rows\")\n",
    "print(f\"\\nFirst few projects:\")\n",
    "print(all_projects.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8c430",
   "metadata": {},
   "source": [
    "## 2. Select Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7d163",
   "metadata": {},
   "source": [
    "### 2.01 Select first 10 Projects or define your own list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e4038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 10 projects:\n",
      "  P119893\n",
      "  P173506\n",
      "  P176731\n",
      "  P507759\n",
      "  P180547\n",
      "  P505856\n",
      "  P181341\n",
      "  P075941\n",
      "  P160708\n",
      "  P153743\n"
     ]
    }
   ],
   "source": [
    "# Select first 10 project IDs\n",
    "selected_projects = all_projects.head(10)[\"project_id\"].tolist()\n",
    "\n",
    "print(f\"Selected {len(selected_projects)} projects:\")\n",
    "for project_id in selected_projects:\n",
    "    print(f\"  {project_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83fa2704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom list (first 10 plus important project)\n",
    "selected_projects = ['P119893', 'P173506', 'P176731', 'P507759', \n",
    "                     'P180547', 'P505856', 'P181341', 'P075941', 'P160708', \n",
    "                     'P153743', 'P511453']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6197bb",
   "metadata": {},
   "source": [
    "### 2.02 Filter Projects DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28fe40dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 11 projects:\n",
      "project_id status                       country\n",
      "   P119893 Closed                      Ethiopia\n",
      "   P173506 Active Congo, Democratic Republic of\n",
      "   P176731 Active                      Ethiopia\n",
      "   P507759 Active                    Mozambique\n",
      "   P180547 Active   Eastern and Southern Africa\n",
      "   P505856 Active                    Seychelles\n",
      "   P181341 Active  Somalia, Federal Republic of\n",
      "   P075941 Closed   Eastern and Southern Africa\n",
      "   P160708 Active    Western and Central Africa\n",
      "   P153743 Closed                         Niger\n",
      "   P511453 Active                        Guinea\n"
     ]
    }
   ],
   "source": [
    "# Filter projects dataframe for selected projects\n",
    "projects_df = all_projects[all_projects[\"project_id\"].isin(selected_projects)]\n",
    "\n",
    "print(f\"Filtered to {len(projects_df)} projects:\")\n",
    "print(projects_df[[\"project_id\", \"status\", \"country\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a1046",
   "metadata": {},
   "source": [
    "### 2.03 Save Selected Projects Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff39ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 11 projects to:\n",
      "  /Users/lauren/repos/PAD2Skills/data/silver/selected_projects_data/selected_projects.csv\n"
     ]
    }
   ],
   "source": [
    "# Save filtered projects to silver directory\n",
    "output_dir = project_root / \"data\" / \"silver\" / \"selected_projects_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = output_dir / \"selected_projects.csv\"\n",
    "projects_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(projects_df)} projects to:\")\n",
    "print(f\"  {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a372be",
   "metadata": {},
   "source": [
    "## 3. Pipeline Step #1: Convert PDFs to Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef817c7",
   "metadata": {},
   "source": [
    "### 3.01 Import PDF Conversion Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3988830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauren/repos/PAD2Skills/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PDF conversion module imported\n"
     ]
    }
   ],
   "source": [
    "from src.pdf_conversion.converter import convert_pdfs\n",
    "\n",
    "print(\"✓ PDF conversion module imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c9288",
   "metadata": {},
   "source": [
    "### 3.02 Set Up PDF Conversion Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e74ee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF directory: /Users/lauren/repos/PAD2Skills/data/bronze/pads_pdf\n",
      "Markdown directory: /Users/lauren/repos/PAD2Skills/data/silver/pads_md\n",
      "PDF directory exists: True\n"
     ]
    }
   ],
   "source": [
    "# Set up paths for PDF conversion\n",
    "pdf_dir = project_root / config.paths.raw_pdfs\n",
    "markdown_dir = project_root / config.paths.markdown\n",
    "\n",
    "print(f\"PDF directory: {pdf_dir}\")\n",
    "print(f\"Markdown directory: {markdown_dir}\")\n",
    "print(f\"PDF directory exists: {pdf_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f714f",
   "metadata": {},
   "source": [
    "### 3.03 Convert PDFs for Selected Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a06639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "○ Skipped (already exists): P119893\n",
      "○ Skipped (already exists): P173506\n",
      "○ Skipped (already exists): P176731\n",
      "○ Skipped (already exists): P507759\n",
      "○ Skipped (already exists): P180547\n",
      "○ Skipped (already exists): P505856\n",
      "○ Skipped (already exists): P181341\n",
      "○ Skipped (already exists): P075941\n",
      "○ Skipped (already exists): P160708\n",
      "○ Skipped (already exists): P153743\n",
      "○ Skipped (already exists): P511453\n"
     ]
    }
   ],
   "source": [
    "# Convert PDFs for each selected project\n",
    "for project_id in selected_projects:\n",
    "    pdf_file = pdf_dir / f\"{project_id}_1.pdf\"\n",
    "    \n",
    "    # Skip if PDF doesn't exist\n",
    "    if not pdf_file.exists():\n",
    "        print(f\"⚠ PDF not found: {project_id}\")\n",
    "        continue\n",
    "    \n",
    "    # Convert single PDF using the src utility\n",
    "    results = convert_pdfs(\n",
    "        pdf_dir=pdf_dir,\n",
    "        output_dir=markdown_dir,\n",
    "        specific_pdf=pdf_file.name,\n",
    "        overwrite=False,\n",
    "        accurate_tables=True\n",
    "    )\n",
    "    \n",
    "    # Report result\n",
    "    if results[\"converted\"]:\n",
    "        print(f\"✓ Converted: {project_id}\")\n",
    "    elif results[\"skipped\"]:\n",
    "        print(f\"○ Skipped (already exists): {project_id}\")\n",
    "    elif results[\"failed\"]:\n",
    "        error = results[\"failed\"][0][1]\n",
    "        print(f\"✗ Failed: {project_id} - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbdd056",
   "metadata": {},
   "source": [
    "## 4. Pipeline Step #2: Extract Document Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6a7f1",
   "metadata": {},
   "source": [
    "### 4.01 Import Section Extraction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d84ff31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Section extraction module imported\n"
     ]
    }
   ],
   "source": [
    "from src.extraction.extractor import extract_all_sections\n",
    "\n",
    "print(\"✓ Section extraction module imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd158a4f",
   "metadata": {},
   "source": [
    "### 4.02 Set Up Section Extraction Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52ab07e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown directory: /Users/lauren/repos/PAD2Skills/data/silver/pads_md\n",
      "Sections output directory: /Users/lauren/repos/PAD2Skills/data/silver/document_sections\n"
     ]
    }
   ],
   "source": [
    "# Set up paths for section extraction\n",
    "sections_output_dir = project_root / \"data\" / \"silver\" / \"document_sections\"\n",
    "\n",
    "print(f\"Markdown directory: {markdown_dir}\")\n",
    "print(f\"Sections output directory: {sections_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ea40e",
   "metadata": {},
   "source": [
    "### 4.03 Extract Sections for Selected Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9081bc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "○ Skipped (already exists): P119893\n",
      "○ Skipped (already exists): P173506\n",
      "○ Skipped (already exists): P176731\n",
      "○ Skipped (already exists): P507759\n",
      "○ Skipped (already exists): P180547\n",
      "○ Skipped (already exists): P505856\n",
      "○ Skipped (already exists): P181341\n",
      "○ Skipped (already exists): P075941\n",
      "○ Skipped (already exists): P160708\n",
      "○ Skipped (already exists): P153743\n",
      "○ Skipped (already exists): P511453\n"
     ]
    }
   ],
   "source": [
    "# Extract sections for each selected project\n",
    "for project_id in selected_projects:\n",
    "    md_file = markdown_dir / f\"{project_id}_1.md\"\n",
    "    \n",
    "    # Skip if markdown doesn't exist\n",
    "    if not md_file.exists():\n",
    "        print(f\"⚠ Markdown not found: {project_id}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract sections using the src utility\n",
    "    results = extract_all_sections(\n",
    "        markdown_dir=markdown_dir,\n",
    "        output_dir=sections_output_dir,\n",
    "        specific_file=md_file.name,\n",
    "        overwrite=False\n",
    "    )\n",
    "    \n",
    "    # Report result\n",
    "    if results[\"extracted\"]:\n",
    "        print(f\"✓ Extracted sections: {project_id}\")\n",
    "    elif results[\"skipped\"]:\n",
    "        print(f\"○ Skipped (already exists): {project_id}\")\n",
    "    elif results[\"failed\"]:\n",
    "        error = results[\"failed\"][0][1]\n",
    "        print(f\"✗ Failed: {project_id} - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced715e",
   "metadata": {},
   "source": [
    "## 5. Pipeline Step #3: Extract Abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46fa3d2",
   "metadata": {},
   "source": [
    "### 5.01 Import Abbreviation Extraction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0a529ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Abbreviation extraction module imported\n"
     ]
    }
   ],
   "source": [
    "from src.extraction.extractor import extract_all_abbreviations\n",
    "\n",
    "print(\"✓ Abbreviation extraction module imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10e1ff",
   "metadata": {},
   "source": [
    "### 5.02 Set Up Abbreviation Extraction Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e09f0edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown directory: /Users/lauren/repos/PAD2Skills/data/silver/pads_md\n",
      "Abbreviations output directory: /Users/lauren/repos/PAD2Skills/data/silver/abbreviations_md\n"
     ]
    }
   ],
   "source": [
    "# Set up paths for abbreviation extraction\n",
    "abbreviations_output_dir = project_root / \"data\" / \"silver\" / \"abbreviations_md\"\n",
    "\n",
    "print(f\"Markdown directory: {markdown_dir}\")\n",
    "print(f\"Abbreviations output directory: {abbreviations_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6d545",
   "metadata": {},
   "source": [
    "### 5.03 Extract Abbreviations for Selected Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45225a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "○ Skipped (already exists): P119893\n",
      "○ Skipped (already exists): P173506\n",
      "○ Skipped (already exists): P176731\n",
      "○ Skipped (already exists): P507759\n",
      "○ Skipped (already exists): P180547\n",
      "○ Skipped (already exists): P505856\n",
      "○ Skipped (already exists): P181341\n",
      "○ Skipped (already exists): P075941\n",
      "○ Skipped (already exists): P160708\n",
      "○ Skipped (already exists): P153743\n",
      "○ Skipped (already exists): P511453\n"
     ]
    }
   ],
   "source": [
    "# Extract abbreviations for each selected project\n",
    "for project_id in selected_projects:\n",
    "    md_file = markdown_dir / f\"{project_id}_1.md\"\n",
    "    \n",
    "    # Skip if markdown doesn't exist\n",
    "    if not md_file.exists():\n",
    "        print(f\"⚠ Markdown not found: {project_id}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract abbreviations using the src utility\n",
    "    results = extract_all_abbreviations(\n",
    "        markdown_dir=markdown_dir,\n",
    "        output_dir=abbreviations_output_dir,\n",
    "        specific_file=md_file.name,\n",
    "        overwrite=False\n",
    "    )\n",
    "    \n",
    "    # Report result\n",
    "    if results[\"extracted\"]:\n",
    "        print(f\"✓ Extracted abbreviations: {project_id}\")\n",
    "    elif results[\"skipped\"]:\n",
    "        print(f\"○ Skipped (already exists): {project_id}\")\n",
    "    elif results[\"failed\"]:\n",
    "        error = results[\"failed\"][0][1]\n",
    "        print(f\"✗ Failed: {project_id} - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ca10f",
   "metadata": {},
   "source": [
    "## 6. Pipeline Step #4: Create Chunked Markdown Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117fdb18",
   "metadata": {},
   "source": [
    "### 6.01 Import Chunking Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9703f8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunking module imported\n"
     ]
    }
   ],
   "source": [
    "from src.extraction.extractor import create_chunks\n",
    "\n",
    "print(\"✓ Chunking module imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59b49c",
   "metadata": {},
   "source": [
    "### 6.02 Set Up Chunking Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aadddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown directory: /Users/lauren/repos/PAD2Skills/data/silver/pads_md\n",
      "Sections directory: /Users/lauren/repos/PAD2Skills/data/silver/document_sections\n",
      "Chunks output directory: /Users/lauren/repos/PAD2Skills/data/silver/pads_md_chunks\n"
     ]
    }
   ],
   "source": [
    "# Set up paths for chunking\n",
    "chunks_output_dir = project_root / \"data\" / \"silver\" / \"pads_md_chunks\"\n",
    "\n",
    "print(f\"Markdown directory: {markdown_dir}\")\n",
    "print(f\"Sections directory: {sections_output_dir}\")\n",
    "print(f\"Chunks output directory: {chunks_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb546025",
   "metadata": {},
   "source": [
    "### 6.03 Create Chunks for Selected Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4bad97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created chunks: P119893 (11 chunks)\n",
      "✓ Created chunks: P173506 (10 chunks)\n",
      "✓ Created chunks: P176731 (11 chunks)\n",
      "✓ Created chunks: P507759 (7 chunks)\n",
      "✓ Created chunks: P180547 (16 chunks)\n",
      "✓ Created chunks: P505856 (6 chunks)\n",
      "✓ Created chunks: P181341 (16 chunks)\n",
      "✓ Created chunks: P075941 (17 chunks)\n",
      "✓ Created chunks: P160708 (21 chunks)\n",
      "✓ Created chunks: P153743 (11 chunks)\n",
      "✓ Created chunks: P511453 (8 chunks)\n"
     ]
    }
   ],
   "source": [
    "# Create chunks for each selected project\n",
    "for project_id in selected_projects:\n",
    "    md_file = markdown_dir / f\"{project_id}_1.md\"\n",
    "    \n",
    "    # Skip if markdown doesn't exist\n",
    "    if not md_file.exists():\n",
    "        print(f\"⚠ Markdown not found: {project_id}\")\n",
    "        continue\n",
    "    \n",
    "    # Create chunks using the src utility\n",
    "    results = create_chunks(\n",
    "        markdown_dir=markdown_dir,\n",
    "        sections_dir=sections_output_dir,\n",
    "        output_dir=chunks_output_dir,\n",
    "        specific_file=md_file.name,\n",
    "        overwrite=False\n",
    "    )\n",
    "    \n",
    "    # Report result\n",
    "    if results[\"chunked\"]:\n",
    "        # Count chunks created for this project\n",
    "        chunk_files = list(chunks_output_dir.glob(f\"{project_id}_*.md\"))\n",
    "        print(f\"✓ Created chunks: {project_id} ({len(chunk_files)} chunks)\")\n",
    "    elif results[\"skipped\"]:\n",
    "        print(f\"○ Skipped (no sections or already exists): {project_id}\")\n",
    "    elif results[\"failed\"]:\n",
    "        error = results[\"failed\"][0][1]\n",
    "        print(f\"✗ Failed: {project_id} - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5aecd1",
   "metadata": {},
   "source": [
    "## 7. Pipeline Step #5: Generate PAD Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb5a408",
   "metadata": {},
   "source": [
    "### 7.01 Import Summary Generation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "370b968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Summary generation module imported\n"
     ]
    }
   ],
   "source": [
    "from src.extraction.summarizer import generate_all_summaries\n",
    "\n",
    "print(\"✓ Summary generation module imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5368654",
   "metadata": {},
   "source": [
    "### 7.02 Set Up Summary Generation Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b6efcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks directory: /Users/lauren/repos/PAD2Skills/data/silver/pads_md_chunks\n",
      "Abbreviations directory: /Users/lauren/repos/PAD2Skills/data/silver/abbreviations_md\n",
      "Summaries output directory: /Users/lauren/repos/PAD2Skills/data/silver/pad_summaries\n"
     ]
    }
   ],
   "source": [
    "# Set up paths for summary generation\n",
    "summaries_output_dir = project_root / \"data\" / \"silver\" / \"pad_summaries\"\n",
    "\n",
    "print(f\"Chunks directory: {chunks_output_dir}\")\n",
    "print(f\"Abbreviations directory: {abbreviations_output_dir}\")\n",
    "print(f\"Summaries output directory: {summaries_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf19b6",
   "metadata": {},
   "source": [
    "### 7.03 Generate Summaries for Selected Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7b71d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:37:17,383 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated summary: P119893 (286 words)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:38:43,691 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated summary: P173506 (286 words)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:39:39,986 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated summary: P176731 (247 words)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:41:17,405 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated summary: P507759 (259 words)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:42:32,319 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated summary: P180547 (250 words)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:43:20,673 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated summary: P505856 (277 words)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:44:17,072 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated summary: P181341 (250 words)\n",
      "○ Skipped (already exists): P075941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:44:52,179 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated summary: P160708 (269 words)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:45:39,144 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated summary: P153743 (268 words)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:46:25,189 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated summary: P511453 (265 words)\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries for each selected project\n",
    "for project_id in selected_projects:\n",
    "    # Check if chunks exist for this project\n",
    "    chunk_files = list(chunks_output_dir.glob(f\"{project_id}_*.md\"))\n",
    "    \n",
    "    if not chunk_files:\n",
    "        print(f\"⚠ No chunks found: {project_id}\")\n",
    "        continue\n",
    "    \n",
    "    # Generate summary using the src utility\n",
    "    results = generate_all_summaries(\n",
    "        chunks_dir=chunks_output_dir,\n",
    "        output_dir=summaries_output_dir,\n",
    "        abbr_dir=abbreviations_output_dir,\n",
    "        specific_project=project_id,\n",
    "        num_chunks=4,\n",
    "        overwrite=False\n",
    "    )\n",
    "    \n",
    "    # Report result\n",
    "    if results[\"generated\"]:\n",
    "        summary_file = summaries_output_dir / f\"{project_id}_summary.txt\"\n",
    "        summary_text = summary_file.read_text(encoding=\"utf-8\")\n",
    "        word_count = len(summary_text.split())\n",
    "        print(f\"✓ Generated summary: {project_id} ({word_count} words)\")\n",
    "    elif results[\"skipped\"]:\n",
    "        print(f\"○ Skipped (already exists): {project_id}\")\n",
    "    elif results[\"failed\"]:\n",
    "        error = results[\"failed\"][0][1]\n",
    "        print(f\"✗ Failed: {project_id} - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b2008",
   "metadata": {},
   "source": [
    "## 8. Pipeline Step #5: Extract Occupations and Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c740f",
   "metadata": {},
   "source": [
    "### 8.01 Import Occupations Extraction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b52a508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Occupations extraction module imported\n"
     ]
    }
   ],
   "source": [
    "from src.extraction.occupations_extractor import extract_all_occupations\n",
    "\n",
    "print(\"✓ Occupations extraction module imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f40e64",
   "metadata": {},
   "source": [
    "### 8.02 Set Up Occupations Extraction Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5aa0ca3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks directory: /Users/lauren/repos/PAD2Skills/data/silver/pads_md_chunks\n",
      "Abbreviations directory: /Users/lauren/repos/PAD2Skills/data/silver/abbreviations_md\n",
      "Occupations output directory: /Users/lauren/repos/PAD2Skills/data/silver/occupations_skills_json\n"
     ]
    }
   ],
   "source": [
    "# Set up paths for occupations extraction\n",
    "occupations_output_dir = project_root / \"data\" / \"silver\" / \"occupations_skills_json\"\n",
    "\n",
    "print(f\"Chunks directory: {chunks_output_dir}\")\n",
    "print(f\"Abbreviations directory: {abbreviations_output_dir}\")\n",
    "print(f\"Occupations output directory: {occupations_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653372cb",
   "metadata": {},
   "source": [
    "### 8.03 Extract Occupations for Selected Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6a35fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 23:23:03,589 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:23:52,024 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:24:36,063 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:26:09,853 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:28:37,932 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:29:39,476 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:32:30,695 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:34:29,994 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:36:11,679 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:38:08,470 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:39:52,920 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted occupations: P119893 (11 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 23:42:05,842 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:45:44,163 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:48:07,733 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:50:06,418 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:50:11,726 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:51:31,207 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:53:50,884 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:56:12,987 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:57:10,571 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:57:14,974 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted occupations: P173506 (10 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 23:59:07,812 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 23:59:13,454 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:01:58,386 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:03:55,186 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:06:59,511 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:07:15,221 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:09:17,856 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:11:00,963 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:12:53,208 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:14:13,494 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:15:31,420 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted occupations: P176731 (11 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 00:18:07,996 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:22:33,985 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:24:11,421 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:26:45,076 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:26:58,850 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:28:11,250 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:30:16,281 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted occupations: P507759 (7 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 00:31:58,013 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:33:48,739 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:36:33,478 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:39:07,846 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:41:53,794 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:43:45,819 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:45:04,527 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:48:39,278 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:50:22,943 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:52:21,114 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:52:28,285 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:53:41,297 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:55:04,245 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:55:23,985 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 00:58:27,915 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:00:28,277 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted occupations: P180547 (16 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 01:02:23,721 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:03:57,082 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:07:48,309 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:09:51,163 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:12:09,132 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:13:08,939 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted occupations: P505856 (6 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 01:15:13,249 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:17:19,337 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:24:58,018 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:27:14,261 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:29:31,744 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:30:57,208 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:32:37,812 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:36:07,102 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:38:11,341 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:42:01,238 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:43:00,774 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:44:01,450 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:44:57,884 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:45:45,396 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:48:14,601 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:52:36,042 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted occupations: P181341 (16 files)\n",
      "○ Skipped (already exists): P075941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 01:54:25,410 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:55:45,444 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:57:25,350 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 01:59:58,320 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:01:24,735 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:01:39,560 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:02:56,195 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:03:00,905 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:03:08,073 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:04:28,225 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:04:36,549 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:07:53,083 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:07:56,480 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:10:26,153 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:12:53,087 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:14:32,118 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:16:50,380 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:20:53,280 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:23:15,927 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:24:33,240 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:25:36,525 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted occupations: P160708 (21 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 02:26:50,972 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:28:40,848 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:29:25,451 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:32:13,036 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:34:29,630 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:35:31,530 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:37:18,985 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:38:39,559 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:40:36,183 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:43:12,243 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:44:51,471 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted occupations: P153743 (11 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 02:46:14,622 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:50:05,948 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:52:44,510 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:54:38,441 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:57:35,356 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 02:59:51,587 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 03:02:07,886 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-03 03:02:13,209 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted occupations: P511453 (8 files)\n"
     ]
    }
   ],
   "source": [
    "# Extract occupations for each selected project\n",
    "for project_id in selected_projects:\n",
    "    # Check if chunks exist for this project\n",
    "    chunk_files = list(chunks_output_dir.glob(f\"{project_id}_*.md\"))\n",
    "    \n",
    "    if not chunk_files:\n",
    "        print(f\"⚠ No chunks found: {project_id}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract occupations using the src utility\n",
    "    results = extract_all_occupations(\n",
    "        chunks_dir=chunks_output_dir,\n",
    "        output_dir=occupations_output_dir,\n",
    "        abbr_dir=abbreviations_output_dir,\n",
    "        specific_project=project_id,\n",
    "        overwrite=False\n",
    "    )\n",
    "    \n",
    "    # Report result\n",
    "    if results[\"generated\"]:\n",
    "        occupation_files = list(occupations_output_dir.glob(f\"{project_id}_*_occupations.json\"))\n",
    "        print(f\"✓ Extracted occupations: {project_id} ({len(results['generated'])} files)\")\n",
    "    elif results[\"skipped\"]:\n",
    "        print(f\"○ Skipped (already exists): {project_id}\")\n",
    "    elif results[\"failed\"]:\n",
    "        error = results[\"failed\"][0][1] if results[\"failed\"] else \"Unknown error\"\n",
    "        print(f\"✗ Failed: {project_id} - {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run occupation extraction for P075941 with overwrite\n",
    "project_id = \"P075941\"\n",
    "\n",
    "results = extract_all_occupations(\n",
    "    chunks_dir=chunks_output_dir,\n",
    "    output_dir=occupations_output_dir,\n",
    "    abbr_dir=abbreviations_output_dir,\n",
    "    specific_project=project_id,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "if results[\"generated\"]:\n",
    "    print(f\"✓ Re-extracted occupations: {project_id} ({len(results['generated'])} files)\")\n",
    "elif results[\"failed\"]:\n",
    "    error = results[\"failed\"][0][1] if results[\"failed\"] else \"Unknown error\"\n",
    "    print(f\"✗ Failed: {project_id} - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2fad0",
   "metadata": {},
   "source": [
    "## 9. Pipeline Step #6: Match PAD Occupations to ESCO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e258658",
   "metadata": {},
   "source": [
    "### 9.01 Import ESCO Matching Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a06f1061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauren/repos/PAD2Skills/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ESCO matching modules imported\n"
     ]
    }
   ],
   "source": [
    "from src.matching.esco_prepare import prepare_esco_data\n",
    "from src.matching.pad_matcher import match_pad_to_esco\n",
    "\n",
    "print(\"✓ ESCO matching modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c59e6",
   "metadata": {},
   "source": [
    "### 9.02 Prepare ESCO Data (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8253aa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "○ ESCO data already prepared (skipping)\n"
     ]
    }
   ],
   "source": [
    "# Prepare ESCO data with embeddings (only needs to be run once)\n",
    "esco_csv = project_root / \"data\" / \"bronze\" / \"esco\" / \"occupations_en.csv\"\n",
    "esco_relations_csv = project_root / \"data\" / \"bronze\" / \"esco\" / \"occupationSkillRelations_en.csv\"\n",
    "esco_output_csv = project_root / \"data\" / \"silver\" / \"esco_occupations_prepared.csv\"\n",
    "esco_embeddings_file = project_root / \"data\" / \"silver\" / \"embeddings\" / \"esco_embeddings.npy\"\n",
    "\n",
    "# Check if ESCO data is already prepared\n",
    "if esco_output_csv.exists() and esco_embeddings_file.exists():\n",
    "    print(\"○ ESCO data already prepared (skipping)\")\n",
    "else:\n",
    "    print(\"Preparing ESCO data with embeddings...\")\n",
    "    prepare_esco_data(\n",
    "        esco_csv=esco_csv,\n",
    "        esco_relations_csv=esco_relations_csv,\n",
    "        output_csv=esco_output_csv,\n",
    "        embeddings_file=esco_embeddings_file,\n",
    "        model_name=\"intfloat/e5-small-v2\",\n",
    "        overwrite_embeddings=False\n",
    "    )\n",
    "    print(\"✓ ESCO data prepared successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a05ca",
   "metadata": {},
   "source": [
    "### 9.03 Set Up PAD Matching Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ce4e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD occupations directory: /Users/lauren/repos/PAD2Skills/data/silver/occupations_skills_json\n",
      "ESCO prepared CSV: /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv\n",
      "ESCO embeddings: /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy\n",
      "Matching CSV output: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv\n",
      "Matching JSON output: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n"
     ]
    }
   ],
   "source": [
    "# Set up paths for PAD matching\n",
    "pad_occupations_dir = project_root / \"data\" / \"silver\" / \"occupations_skills_json\"\n",
    "esco_matching_csv_dir = project_root / \"data\" / \"silver\" / \"esco_matching_csv\"\n",
    "esco_matching_json_dir = project_root / \"data\" / \"silver\" / \"esco_matching_json\"\n",
    "\n",
    "print(f\"PAD occupations directory: {pad_occupations_dir}\")\n",
    "print(f\"ESCO prepared CSV: {esco_output_csv}\")\n",
    "print(f\"ESCO embeddings: {esco_embeddings_file}\")\n",
    "print(f\"Matching CSV output: {esco_matching_csv_dir}\")\n",
    "print(f\"Matching JSON output: {esco_matching_json_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a1d22",
   "metadata": {},
   "source": [
    "### 9.04 Match PAD Occupations to ESCO for Selected Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "710240c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing CSV: P119893_esco_matches.csv\n",
      "Deleted 2 existing JSON chunk files\n",
      "Loading ESCO data from /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv...\n",
      "✓ Loaded 3,037 ESCO occupations\n",
      "Loading ESCO embeddings from /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy...\n",
      "✓ Loaded embeddings: shape=(3037, 384), size=4.45 MB\n",
      "Loading PAD occupations for project P119893...\n",
      "✓ Loaded 134 PAD occupation extractions\n",
      "Loading model: intfloat/e5-small-v2...\n",
      "  Max sequence length: 512\n",
      "  Embedding dimension: 384\n",
      "Encoding 134 PAD occupation queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded PAD occupations: shape=(134, 384), size=0.20 MB\n",
      "Computing similarity scores...\n",
      "✓ Computed similarity matrix: shape=(134, 3037)\n",
      "  Score range: [0.6596, 0.8911], mean=0.7519\n",
      "Finding top 20 matches for each PAD occupation...\n",
      "✓ Found top 20 matches for all 134 PAD occupations\n",
      "✓ Created results DataFrame: shape=(134, 109)\n",
      "✓ Saved CSV results to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/P119893_esco_matches.csv\n",
      "  File size: 1362.26 KB\n",
      "✓ Saved diagnostics to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/diagnostics/P119893_esco_matches_diagnostics.csv\n",
      "  File size: 107.79 KB\n",
      "Splitting 134 records into 2 chunk(s) of up to 75 records\n",
      "  ✓ Saved chunk 1/2: P119893_000-074_esco_matches.json (75 records, 431.92 KB)\n",
      "  ✓ Saved chunk 2/2: P119893_075-133_esco_matches.json (59 records, 343.23 KB)\n",
      "✓ Saved 2 JSON file(s) to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n",
      "✓ Matched occupations: P119893\n",
      "Deleted existing CSV: P173506_esco_matches.csv\n",
      "Deleted 2 existing JSON chunk files\n",
      "Loading ESCO data from /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv...\n",
      "✓ Loaded 3,037 ESCO occupations\n",
      "Loading ESCO embeddings from /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy...\n",
      "✓ Loaded embeddings: shape=(3037, 384), size=4.45 MB\n",
      "Loading PAD occupations for project P173506...\n",
      "✓ Loaded 138 PAD occupation extractions\n",
      "Loading model: intfloat/e5-small-v2...\n",
      "  Max sequence length: 512\n",
      "  Embedding dimension: 384\n",
      "Encoding 138 PAD occupation queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded PAD occupations: shape=(138, 384), size=0.20 MB\n",
      "Computing similarity scores...\n",
      "✓ Computed similarity matrix: shape=(138, 3037)\n",
      "  Score range: [0.6670, 0.8962], mean=0.7546\n",
      "Finding top 20 matches for each PAD occupation...\n",
      "✓ Found top 20 matches for all 138 PAD occupations\n",
      "✓ Created results DataFrame: shape=(138, 109)\n",
      "✓ Saved CSV results to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/P173506_esco_matches.csv\n",
      "  File size: 1404.18 KB\n",
      "✓ Saved diagnostics to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/diagnostics/P173506_esco_matches_diagnostics.csv\n",
      "  File size: 119.31 KB\n",
      "Splitting 138 records into 2 chunk(s) of up to 75 records\n",
      "  ✓ Saved chunk 1/2: P173506_000-074_esco_matches.json (75 records, 431.47 KB)\n",
      "  ✓ Saved chunk 2/2: P173506_075-137_esco_matches.json (63 records, 373.56 KB)\n",
      "✓ Saved 2 JSON file(s) to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n",
      "✓ Matched occupations: P173506\n",
      "Deleted existing CSV: P176731_esco_matches.csv\n",
      "Deleted 2 existing JSON chunk files\n",
      "Loading ESCO data from /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv...\n",
      "✓ Loaded 3,037 ESCO occupations\n",
      "Loading ESCO embeddings from /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy...\n",
      "✓ Loaded embeddings: shape=(3037, 384), size=4.45 MB\n",
      "Loading PAD occupations for project P176731...\n",
      "✓ Loaded 113 PAD occupation extractions\n",
      "Loading model: intfloat/e5-small-v2...\n",
      "  Max sequence length: 512\n",
      "  Embedding dimension: 384\n",
      "Encoding 113 PAD occupation queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded PAD occupations: shape=(113, 384), size=0.17 MB\n",
      "Computing similarity scores...\n",
      "✓ Computed similarity matrix: shape=(113, 3037)\n",
      "  Score range: [0.6730, 0.8874], mean=0.7532\n",
      "Finding top 20 matches for each PAD occupation...\n",
      "✓ Found top 20 matches for all 113 PAD occupations\n",
      "✓ Created results DataFrame: shape=(113, 109)\n",
      "✓ Saved CSV results to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/P176731_esco_matches.csv\n",
      "  File size: 1145.10 KB\n",
      "✓ Saved diagnostics to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/diagnostics/P176731_esco_matches_diagnostics.csv\n",
      "  File size: 91.65 KB\n",
      "Splitting 113 records into 2 chunk(s) of up to 75 records\n",
      "  ✓ Saved chunk 1/2: P176731_000-074_esco_matches.json (75 records, 431.31 KB)\n",
      "  ✓ Saved chunk 2/2: P176731_075-112_esco_matches.json (38 records, 220.33 KB)\n",
      "✓ Saved 2 JSON file(s) to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n",
      "✓ Matched occupations: P176731\n",
      "Deleted existing CSV: P507759_esco_matches.csv\n",
      "Deleted 2 existing JSON chunk files\n",
      "Loading ESCO data from /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv...\n",
      "✓ Loaded 3,037 ESCO occupations\n",
      "Loading ESCO embeddings from /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy...\n",
      "✓ Loaded embeddings: shape=(3037, 384), size=4.45 MB\n",
      "Loading PAD occupations for project P507759...\n",
      "✓ Loaded 89 PAD occupation extractions\n",
      "Loading model: intfloat/e5-small-v2...\n",
      "  Max sequence length: 512\n",
      "  Embedding dimension: 384\n",
      "Encoding 89 PAD occupation queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded PAD occupations: shape=(89, 384), size=0.13 MB\n",
      "Computing similarity scores...\n",
      "✓ Computed similarity matrix: shape=(89, 3037)\n",
      "  Score range: [0.6767, 0.8785], mean=0.7549\n",
      "Finding top 20 matches for each PAD occupation...\n",
      "✓ Found top 20 matches for all 89 PAD occupations\n",
      "✓ Created results DataFrame: shape=(89, 109)\n",
      "✓ Saved CSV results to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/P507759_esco_matches.csv\n",
      "  File size: 906.52 KB\n",
      "✓ Saved diagnostics to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/diagnostics/P507759_esco_matches_diagnostics.csv\n",
      "  File size: 79.05 KB\n",
      "Splitting 89 records into 2 chunk(s) of up to 75 records\n",
      "  ✓ Saved chunk 1/2: P507759_000-074_esco_matches.json (75 records, 438.46 KB)\n",
      "  ✓ Saved chunk 2/2: P507759_075-088_esco_matches.json (14 records, 82.46 KB)\n",
      "✓ Saved 2 JSON file(s) to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n",
      "✓ Matched occupations: P507759\n",
      "Deleted existing CSV: P180547_esco_matches.csv\n",
      "Deleted 3 existing JSON chunk files\n",
      "Loading ESCO data from /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv...\n",
      "✓ Loaded 3,037 ESCO occupations\n",
      "Loading ESCO embeddings from /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy...\n",
      "✓ Loaded embeddings: shape=(3037, 384), size=4.45 MB\n",
      "Loading PAD occupations for project P180547...\n",
      "✓ Loaded 213 PAD occupation extractions\n",
      "Loading model: intfloat/e5-small-v2...\n",
      "  Max sequence length: 512\n",
      "  Embedding dimension: 384\n",
      "Encoding 213 PAD occupation queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded PAD occupations: shape=(213, 384), size=0.31 MB\n",
      "Computing similarity scores...\n",
      "✓ Computed similarity matrix: shape=(213, 3037)\n",
      "  Score range: [0.6681, 0.8842], mean=0.7497\n",
      "Finding top 20 matches for each PAD occupation...\n",
      "✓ Found top 20 matches for all 213 PAD occupations\n",
      "✓ Created results DataFrame: shape=(213, 109)\n",
      "✓ Saved CSV results to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/P180547_esco_matches.csv\n",
      "  File size: 2223.44 KB\n",
      "✓ Saved diagnostics to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/diagnostics/P180547_esco_matches_diagnostics.csv\n",
      "  File size: 204.78 KB\n",
      "Splitting 213 records into 3 chunk(s) of up to 75 records\n",
      "  ✓ Saved chunk 1/3: P180547_000-074_esco_matches.json (75 records, 444.51 KB)\n",
      "  ✓ Saved chunk 2/3: P180547_075-149_esco_matches.json (75 records, 451.93 KB)\n",
      "  ✓ Saved chunk 3/3: P180547_150-212_esco_matches.json (63 records, 377.26 KB)\n",
      "✓ Saved 3 JSON file(s) to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n",
      "✓ Matched occupations: P180547\n",
      "Deleted existing CSV: P505856_esco_matches.csv\n",
      "Deleted 1 existing JSON chunk files\n",
      "Loading ESCO data from /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv...\n",
      "✓ Loaded 3,037 ESCO occupations\n",
      "Loading ESCO embeddings from /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy...\n",
      "✓ Loaded embeddings: shape=(3037, 384), size=4.45 MB\n",
      "Loading PAD occupations for project P505856...\n",
      "✓ Loaded 59 PAD occupation extractions\n",
      "Loading model: intfloat/e5-small-v2...\n",
      "  Max sequence length: 512\n",
      "  Embedding dimension: 384\n",
      "Encoding 59 PAD occupation queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded PAD occupations: shape=(59, 384), size=0.09 MB\n",
      "Computing similarity scores...\n",
      "✓ Computed similarity matrix: shape=(59, 3037)\n",
      "  Score range: [0.6691, 0.8621], mean=0.7515\n",
      "Finding top 20 matches for each PAD occupation...\n",
      "✓ Found top 20 matches for all 59 PAD occupations\n",
      "✓ Created results DataFrame: shape=(59, 109)\n",
      "✓ Saved CSV results to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/P505856_esco_matches.csv\n",
      "  File size: 610.08 KB\n",
      "✓ Saved diagnostics to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/diagnostics/P505856_esco_matches_diagnostics.csv\n",
      "  File size: 49.19 KB\n",
      "Splitting 59 records into 1 chunk(s) of up to 75 records\n",
      "  ✓ Saved chunk 1/1: P505856_000-058_esco_matches.json (59 records, 343.74 KB)\n",
      "✓ Saved 1 JSON file(s) to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n",
      "✓ Matched occupations: P505856\n",
      "Deleted existing CSV: P181341_esco_matches.csv\n",
      "Deleted 3 existing JSON chunk files\n",
      "Loading ESCO data from /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv...\n",
      "✓ Loaded 3,037 ESCO occupations\n",
      "Loading ESCO embeddings from /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy...\n",
      "✓ Loaded embeddings: shape=(3037, 384), size=4.45 MB\n",
      "Loading PAD occupations for project P181341...\n",
      "✓ Loaded 216 PAD occupation extractions\n",
      "Loading model: intfloat/e5-small-v2...\n",
      "  Max sequence length: 512\n",
      "  Embedding dimension: 384\n",
      "Encoding 216 PAD occupation queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:06<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded PAD occupations: shape=(216, 384), size=0.32 MB\n",
      "Computing similarity scores...\n",
      "✓ Computed similarity matrix: shape=(216, 3037)\n",
      "  Score range: [0.6514, 0.8861], mean=0.7501\n",
      "Finding top 20 matches for each PAD occupation...\n",
      "✓ Found top 20 matches for all 216 PAD occupations\n",
      "✓ Created results DataFrame: shape=(216, 109)\n",
      "✓ Saved CSV results to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/P181341_esco_matches.csv\n",
      "  File size: 2252.77 KB\n",
      "✓ Saved diagnostics to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/diagnostics/P181341_esco_matches_diagnostics.csv\n",
      "  File size: 199.33 KB\n",
      "Splitting 216 records into 3 chunk(s) of up to 75 records\n",
      "  ✓ Saved chunk 1/3: P181341_000-074_esco_matches.json (75 records, 440.64 KB)\n",
      "  ✓ Saved chunk 2/3: P181341_075-149_esco_matches.json (75 records, 444.46 KB)\n",
      "  ✓ Saved chunk 3/3: P181341_150-215_esco_matches.json (66 records, 394.77 KB)\n",
      "✓ Saved 3 JSON file(s) to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n",
      "✓ Matched occupations: P181341\n",
      "Deleted existing CSV: P075941_esco_matches.csv\n",
      "Deleted 3 existing JSON chunk files\n",
      "Loading ESCO data from /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv...\n",
      "✓ Loaded 3,037 ESCO occupations\n",
      "Loading ESCO embeddings from /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy...\n",
      "✓ Loaded embeddings: shape=(3037, 384), size=4.45 MB\n",
      "Loading PAD occupations for project P075941...\n",
      "✓ Loaded 164 PAD occupation extractions\n",
      "Loading model: intfloat/e5-small-v2...\n",
      "  Max sequence length: 512\n",
      "  Embedding dimension: 384\n",
      "Encoding 164 PAD occupation queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded PAD occupations: shape=(164, 384), size=0.24 MB\n",
      "Computing similarity scores...\n",
      "✓ Computed similarity matrix: shape=(164, 3037)\n",
      "  Score range: [0.6324, 0.8967], mean=0.7528\n",
      "Finding top 20 matches for each PAD occupation...\n",
      "✓ Found top 20 matches for all 164 PAD occupations\n",
      "✓ Created results DataFrame: shape=(164, 109)\n",
      "✓ Saved CSV results to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/P075941_esco_matches.csv\n",
      "  File size: 1647.33 KB\n",
      "✓ Saved diagnostics to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/diagnostics/P075941_esco_matches_diagnostics.csv\n",
      "  File size: 129.90 KB\n",
      "Splitting 164 records into 3 chunk(s) of up to 75 records\n",
      "  ✓ Saved chunk 1/3: P075941_000-074_esco_matches.json (75 records, 428.20 KB)\n",
      "  ✓ Saved chunk 2/3: P075941_075-149_esco_matches.json (75 records, 429.74 KB)\n",
      "  ✓ Saved chunk 3/3: P075941_150-163_esco_matches.json (14 records, 78.80 KB)\n",
      "✓ Saved 3 JSON file(s) to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n",
      "✓ Matched occupations: P075941\n",
      "Deleted existing CSV: P160708_esco_matches.csv\n",
      "Deleted 3 existing JSON chunk files\n",
      "Loading ESCO data from /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv...\n",
      "✓ Loaded 3,037 ESCO occupations\n",
      "Loading ESCO embeddings from /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy...\n",
      "✓ Loaded embeddings: shape=(3037, 384), size=4.45 MB\n",
      "Loading PAD occupations for project P160708...\n",
      "✓ Loaded 191 PAD occupation extractions\n",
      "Loading model: intfloat/e5-small-v2...\n",
      "  Max sequence length: 512\n",
      "  Embedding dimension: 384\n",
      "Encoding 191 PAD occupation queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:05<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded PAD occupations: shape=(191, 384), size=0.28 MB\n",
      "Computing similarity scores...\n",
      "✓ Computed similarity matrix: shape=(191, 3037)\n",
      "  Score range: [0.6625, 0.8756], mean=0.7498\n",
      "Finding top 20 matches for each PAD occupation...\n",
      "✓ Found top 20 matches for all 191 PAD occupations\n",
      "✓ Created results DataFrame: shape=(191, 109)\n",
      "✓ Saved CSV results to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/P160708_esco_matches.csv\n",
      "  File size: 2009.55 KB\n",
      "✓ Saved diagnostics to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/diagnostics/P160708_esco_matches_diagnostics.csv\n",
      "  File size: 174.14 KB\n",
      "Splitting 191 records into 3 chunk(s) of up to 75 records\n",
      "  ✓ Saved chunk 1/3: P160708_000-074_esco_matches.json (75 records, 439.29 KB)\n",
      "  ✓ Saved chunk 2/3: P160708_075-149_esco_matches.json (75 records, 462.06 KB)\n",
      "  ✓ Saved chunk 3/3: P160708_150-190_esco_matches.json (41 records, 248.32 KB)\n",
      "✓ Saved 3 JSON file(s) to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n",
      "✓ Matched occupations: P160708\n",
      "Deleted existing CSV: P153743_esco_matches.csv\n",
      "Deleted 2 existing JSON chunk files\n",
      "Loading ESCO data from /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv...\n",
      "✓ Loaded 3,037 ESCO occupations\n",
      "Loading ESCO embeddings from /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy...\n",
      "✓ Loaded embeddings: shape=(3037, 384), size=4.45 MB\n",
      "Loading PAD occupations for project P153743...\n",
      "✓ Loaded 127 PAD occupation extractions\n",
      "Loading model: intfloat/e5-small-v2...\n",
      "  Max sequence length: 512\n",
      "  Embedding dimension: 384\n",
      "Encoding 127 PAD occupation queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded PAD occupations: shape=(127, 384), size=0.19 MB\n",
      "Computing similarity scores...\n",
      "✓ Computed similarity matrix: shape=(127, 3037)\n",
      "  Score range: [0.6734, 0.8931], mean=0.7558\n",
      "Finding top 20 matches for each PAD occupation...\n",
      "✓ Found top 20 matches for all 127 PAD occupations\n",
      "✓ Created results DataFrame: shape=(127, 109)\n",
      "✓ Saved CSV results to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/P153743_esco_matches.csv\n",
      "  File size: 1255.94 KB\n",
      "✓ Saved diagnostics to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/diagnostics/P153743_esco_matches_diagnostics.csv\n",
      "  File size: 104.29 KB\n",
      "Splitting 127 records into 2 chunk(s) of up to 75 records\n",
      "  ✓ Saved chunk 1/2: P153743_000-074_esco_matches.json (75 records, 423.85 KB)\n",
      "  ✓ Saved chunk 2/2: P153743_075-126_esco_matches.json (52 records, 289.72 KB)\n",
      "✓ Saved 2 JSON file(s) to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n",
      "✓ Matched occupations: P153743\n",
      "Deleted existing CSV: P511453_esco_matches.csv\n",
      "Deleted 2 existing JSON chunk files\n",
      "Loading ESCO data from /Users/lauren/repos/PAD2Skills/data/silver/esco_occupations_prepared.csv...\n",
      "✓ Loaded 3,037 ESCO occupations\n",
      "Loading ESCO embeddings from /Users/lauren/repos/PAD2Skills/data/silver/embeddings/esco_embeddings.npy...\n",
      "✓ Loaded embeddings: shape=(3037, 384), size=4.45 MB\n",
      "Loading PAD occupations for project P511453...\n",
      "✓ Loaded 118 PAD occupation extractions\n",
      "Loading model: intfloat/e5-small-v2...\n",
      "  Max sequence length: 512\n",
      "  Embedding dimension: 384\n",
      "Encoding 118 PAD occupation queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded PAD occupations: shape=(118, 384), size=0.17 MB\n",
      "Computing similarity scores...\n",
      "✓ Computed similarity matrix: shape=(118, 3037)\n",
      "  Score range: [0.6669, 0.8813], mean=0.7523\n",
      "Finding top 20 matches for each PAD occupation...\n",
      "✓ Found top 20 matches for all 118 PAD occupations\n",
      "✓ Created results DataFrame: shape=(118, 109)\n",
      "✓ Saved CSV results to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/P511453_esco_matches.csv\n",
      "  File size: 1193.23 KB\n",
      "✓ Saved diagnostics to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_csv/diagnostics/P511453_esco_matches_diagnostics.csv\n",
      "  File size: 113.93 KB\n",
      "Splitting 118 records into 2 chunk(s) of up to 75 records\n",
      "  ✓ Saved chunk 1/2: P511453_000-074_esco_matches.json (75 records, 438.05 KB)\n",
      "  ✓ Saved chunk 2/2: P511453_075-117_esco_matches.json (43 records, 251.13 KB)\n",
      "✓ Saved 2 JSON file(s) to: /Users/lauren/repos/PAD2Skills/data/silver/esco_matching_json\n",
      "✓ Matched occupations: P511453\n"
     ]
    }
   ],
   "source": [
    "# Set overwrite parameter\n",
    "overwrite_matching = False  # Set to True to force re-matching\n",
    "\n",
    "# Match PAD occupations to ESCO for each selected project\n",
    "for project_id in selected_projects:\n",
    "    # Check if occupation JSON files exist for this project\n",
    "    occupation_files = list(pad_occupations_dir.glob(f\"{project_id}_*.json\"))\n",
    "    \n",
    "    if not occupation_files:\n",
    "        print(f\"⚠ No occupation files found: {project_id}\")\n",
    "        continue\n",
    "    \n",
    "    # Match PAD occupations to ESCO\n",
    "    try:\n",
    "        match_pad_to_esco(\n",
    "            pad_occupations_dir=pad_occupations_dir,\n",
    "            project_id=project_id,\n",
    "            esco_csv=esco_output_csv,\n",
    "            esco_embeddings=esco_embeddings_file,\n",
    "            output_dir=project_root / \"data\" / \"silver\",\n",
    "            model_name=\"intfloat/e5-small-v2\",\n",
    "            top_k=20,\n",
    "            chunk_size=75,\n",
    "            save_diagnostics=True,\n",
    "            overwrite=overwrite_matching\n",
    "        )\n",
    "        print(f\"✓ Matched occupations: {project_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {project_id} - {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PAD2Skills",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
